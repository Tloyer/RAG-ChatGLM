2022 年底，ChatGPT 震撼上线，大语言模型技术迅速“席卷”了整个社会，人
工智能技术因此迎来了一次重要进展。面对大语言模型的强大性能，我们不禁要
问：支撑这些模型的背后技术究竟是什么？这一问题无疑成为了众多科研人员的
思考焦点。
必须指出的是，大模型技术并不是一蹴而就，其发展历程中先后经历了统计语
言模型、神经网络语言模型、预训练语言模型等多个发展阶段，每一步的发展都凝
结了众多科研工作者的心血与成果。作为大语言模型技术的重要推动者，OpenAI
公司引领了本次技术变革，让我们再次回顾其针对大模型技术的研发历程。2015
年，OpenAI 公司正式创立，开始探索通用人工智能的技术路线。早期的 OpenAI
团队围绕强化学习、多模态、语言模型等几个重要方向进行了深入研究。其中，由
Ilya Sutskever 领导的团队主要关注语言模型的研究。当谷歌 2017 年推出基于注意
力机制的 Transformer 模型后，OpenAI 团队迅速洞察到了其潜在的优越性，认为这
种模型可能是一种大规模可扩展训练的理想架构。基于此，OpenAI 团队开始构建
GPT 系列模型，并于 2018 年推出了第一代 GPT 模型—GPT-1，能够通过“通用文
本训练-特定任务微调”的范式去解决下游任务。接下来，GPT-2 和 GPT-3 模型通
过扩大预训练数据和模型参数规模，显著提升了模型性能，并且确立了基于自然
语言形式的通用任务解决路径。在 GPT-3 的基础上，OpenAI 又通过代码训练、人
类对齐、工具使用等技术对于模型性能不断升级，推出了功能强大的 GPT-3.5 系
列模型。2022 年 11 月，ChatGPT 正式上线，能够以对话形式解决多种任务，使得
用户能够通过网络 API 体验到语言模型的强大功能。2023 年 3 月，OpenAI 推出了
标志性的 GPT-4 模型，将模型能力提升至全新高度，并将其扩展至拥有多模态功
能的 GPT-4V 模型。
反观 GPT 系列模型的发展历程，有两点令人印象深刻。第一点是可拓展的训
练架构与学习范式：Transformer 架构能够拓展到百亿、千亿甚至万亿参数规模，并
且将预训练任务统一为预测下一个词这一通用学习范式；第二点是对于数据质量
与数据规模的重视：不同于 BERT 时代的预训练语言模型，这次大语言模型的成
功与数据有着更为紧密的关系，高质量数据、超大规模数据成为大语言模型的关
键基础。上述的思路看似简单，但能够从早期众多的技术路线中寻找到这条路线，
并且坚定地去执行这条路线，这就是 OpenAI 成功的关键所在。回顾 OpenAI 的早
期论文，实际上早在 GPT-2 的论文中，就深入讨论了基于大规模文本预训练的通
用任务学习范式，让人不禁感叹 OpenAI 团队的技术前瞻性。虽然这种研究模式很
难复制，但是值得我们去思考、学习。
OpenAI 团队自 GPT-3 开始，就很少在公开的材料中提及相关技术细节，很
多技术报告主要是介绍评测相关的内容。到目前为止，关于 GPT 系列模型的核心
技术仍然难以完全解密。虽然有众多公司在尝试复刻 GPT 水平的大语言模型（如
Anthropic、Google 等），但是整体来说，OpenAI 仍然在大模型技术上有着较大的
领先性。根据 Sam Altman 的公开采访介绍，尚未发布的 GPT-5 相比 GPT-4 将会有
重要的技术进步。如果事实如此，那么 GPT-5 的到来将再次拉大了与当前其他大
语言模型的差距，可能意味着人类向着通用人工智能又迈出了重要一步。
相信有很多人都会有一个共同的疑问：为什么 GPT 水平的大模型难训练？关
于为何 GPT 级别的大模型难以训练，许多人可能会首先想到算力的限制。确实，为
了训练百亿级别的高水平大模型，通常需要最少百卡级别的 A100/A800 (80G) 资
源，而为了充分探索训练过程中的各种细节，往往需要有千卡级别的 A100/A800
(80G) 资源作为支撑。而对于千亿、万亿模型来说，所需要耗费的算力资源更是极
为庞大。目前，学术界面临的重大挑战是真正有充足资源去尝试预训练技术的团
队少之又少，因此导致了第一手经验匮乏，难以直接开展相关研究。
大模型训练涉及众多训练的细节，这些细节很多时候无法从已有科研论文中
直接获取。在统计学习时代，可以针对机器学习模型进行非常充分的实证研究，例
如使用栅格搜索参数的最优值、选择核函数、执行交叉验证等。通过广泛的调优
实验，研究人员很快就能积累充足的训练经验，进而形成对于这些统计机器学习
模型的深入理解。但是，大语言模型由于参数众多、组件复杂、训练过程也比较
复杂，早期的实验探索如果不引入任何先验知识，可能会导致指数级增长的实验
数量。然而，现实情况是，很多研究人员并没有足够的资源去完成一次完整的大
规模预训练实验，这使得掌握大模型技术的第一手经验变得尤为困难，更不用说
从零开始探索相关科研问题，极大限制了学术界在此次人工浪潮中所起到的作用。
目前，能力较强的大语言模型基本都源自工业界，这一趋势随着时间的推移可能
会变得更加明显。从第一手经验中“Know-How”，对于科研人员来说非常重要，只
有接触到技术核心，才能真正理解哪些问题是有意义的，并找到解决方案。
令人欣喜的是，无论是在学术界还是工业界，人们都逐渐认识到了“开放”的
重要性，能够看到越来越多的公开的基础模型、技术代码以及学术论文，有力地
推动了大模型技术的“透明化”。只有通过开放和共享，才能汇聚全人类的智慧，
共同推进人工智能技术的发展。实际上，根据现有公开的资料，大模型技术也是
“有章可循”的，如整体训练流程、数据清洗方法、指令微调技术、人类偏好对齐
算法等。根据这些技术，在算力资源支持下，研发人员已经能够较为顺利地完成
大模型的整体训练流程，并取得不错的模型效果。随着更多核心技术的揭示和开
放，大模型技术的“透明化”将进一步提高。
为了更好地整理和传播大模型技术的最新进展与技术体系，我们在 2023 年 3
月发表了大语言模型综述文章《A Survey of Large Language Models》，并不断进行
更新完善。这篇综述文章已经更新到第 13 个版本，包含了 83 页的正文内容，并
收录了 900 余篇参考文献。自英文综述文章上线后，陆续有读者询问是否有对应
的中文版本。为此，我们于 2023 年 8 月发布了该综述（v10）的中文翻译版。在
2023 年 12 月底，为了更好地提供大模型技术的中文参考资料，我们启动了中文书
的编写工作，并且于近日完成初稿。与英文综述文章的定位不同，中文版书籍更
注重为大模型技术的入门读者提供讲解，为此我们在内容上进行了大幅度的更新
与重组，力图展现一个整体的大模型技术框架和路线图。本书适用于具有深度学
习基础的高年级本科生以及低年级研究生使用，可以作为一本入门级的技术书籍。
在准备中文书的过程中，我们广泛阅读了现有的经典论文、相关代码和教材，
从中提炼出核心概念、主流算法与模型，并进行了系统性的组织与介绍。我们对于
每个章节的内容初稿都进行了多次修正，力求表达的清晰性与准确性。然而，在书
写过程中，我们深感自身能力与知识的局限性，尽管已经付出了极大的努力，但
难免会有遗漏或不当之处。本书的初版仅是一个起点，我们计划在网上持续进行
内容的更新和完善，并特别欢迎读者提出宝贵的批评与建议，也会同步在网站上
对于提出宝贵建议的读者进行致谢。我们将编写此书的过程当做一个自身的学习
过程，也希望能够通过本书与读者进行深入交流，向更多的行业同行学习。
总之，大模型技术正处于快速发展阶段，基础原理亟待探索、关键技术亟待
改善。对于科研人员而言，大模型研究工作充满了想象空间，令人为之神往。随
着技术的不断进步与共享开放，我们有理由相信，未来人工智能技术将取得更大
的进展，将在更多领域带来更为深远的影响。
2.4 GPT 系列模型的技术演变
2022 年 11 月底，OpenAI 推出了基于大语言模型的在线对话应用 — ChatGPT。
由于具备出色的人机对话能力和任务解决能力，ChatGPT 一经发布就引发了全社
会对于大语言模型的广泛关注，众多的大语言模型应运而生，并且数量还在不断
增加（图 2.1）。由于 GPT 系列模型具有重要的代表性，本部分内容将针对 GPT 系
列模型的发展历程进行介绍，并且凝练出其中的重要技术创新。
GPT 系列模型的基本原理是训练模型学习恢复预训练文本数据，将广泛的世
界知识压缩到仅包含解码器（Decoder-Only）的 Transformer 模型中，从而使模型
能够学习获得较为全面的能力。其中，两个关键要素是：（I）训练能够准确预测下
一个词的 Transformer （只包含解码器）语言模型；（II）扩展语言模型的规模以及
扩展预训练数据的规模。图 2.2 展示了 GPT 系列模型的技术演进示意图，这里主
要根据 OpenAI 的论文、博客文章和官方 API 说明的信息进行绘制。该图中 实线
表示在两个模型之间的进化路径上存在明确的证据（例如，官方声明新模型是基
于基础模型开发的），而 虚线 表示相对较弱的进化关系。截止到目前，OpenAI 对
大语言模型的研发历程大致可分为四个阶段：早期探索阶段、路线确立阶段、能
力增强阶段以及能力跃升阶段，下面进行具体介绍2。
2.4.1 早期探索
根据对于 Ilya Sutskever（OpenAI 联合创始人、前首席科学家）的采访3，OpenAI
在成立初期就尝试使用语言模型研发人工智能系统，但当时使用的是循环神经网
络 [46]，模型能力和并行训练能力还存在较大的局限性。Transformer 刚刚问世，就
引起了 OpenAI 团队的高度关注，并且将语言模型的研发工作切换到 Transformer
架构上，相继推出了两个初始的 GPT 模型，即 GPT-1 [14] 和 GPT-2 [17]，这两个
早期工作奠定了后续更强大的 GPT 模型（如 GPT-3 和 GPT-4）的研究基础。
• GPT-1. 2017 年，Google 推出 Transformer 模型后，OpenAI 团队马上意识
到这种神经网络架构将显著优于传统序列神经网络的性能，有可能对于研发大型
神经网络产生重要的影响。他们很快着手使用 Transformer 架构研发语言模型，并
于 2018 年发布了第一个 GPT 模型，即 GPT-1，模型名称 GPT 是生成式预训练
2本部分讨论的内容是作者基于调研 OpenAI 发布的论文、博客文章、采访报道和 API 说明文档所形
成的个人理解。
3https://hackernoon.com/an-interview-with-ilya-sutskever-co-founder-of-openai
26
2.4 GPT 系列模型的技术演变
（Generative Pre-Training）的缩写。GPT-1 基于生成式、仅有解码器的 Transformer
架构开发，奠定了 GPT 系列模型的核心架构与基于自然语言文本的预训练方式，
即预测下一个词元。由于当时模型的参数规模还相对较小，模型仍然缺乏通用的
任务求解能力，因而采用了无监督预训练和有监督微调相结合的范式。与 GPT-1
同期发布的预训练语言模型是大名鼎鼎的 BERT 模型。BERT 与 GPT-1 虽然都采
用了基于 Transformer 架构的预训练学习方式，但是它主要面向自然语言理解任务
（Natural Language Understanding, NLU），为此只保留了 Transformer 中的编码器，其
中 BERT-Large 模型在众多的自然语言理解任务上取得了非常重要的提升，成为当
时备受瞩目的“明星模型”。可以说，BERT 当时引领了自然语言处理社区的研究
浪潮，涌现了大量针对它改进与探索的工作。由于 GPT-1 模型规模实际上与小规
模的 BERT-Base 模型相当（100M 左右参数），在公开评测数据集合上的性能尚不
能达到当时众多竞争模型中的最优效果，没有引起学术界的足够关注。
• GPT-2. GPT-2 沿用了 GPT-1 的类似架构，将参数规模扩大到 1.5B，并使用
大规模网页数据集 WebText 进行预训练。与 GPT-1 不同，GPT-2 旨在探索通过扩
大模型参数规模来提升模型性能，并且尝试去除针对特定任务所需要的微调环节。
这点在 GPT-2 的论文 [17] 中得到了着重论述，它试图使用无监督预训练的语言模
型来解决各种下游任务，进而不需要使用标注数据进行显式的模型微调。形式化
来说，多任务学习（Multi-task Learning）可以通过一种较为通用的概率形式刻画，
即 𝑃(output|input, task)——根据输入和任务信息来预测输出。为了建立通用的多
任务学习框架，GPT 系列模型将输入、输出和任务信息都通过自然语言形式进行
描述，进而后续任务的求解过程就可以看作是任务方案（或答案）的文本生成问
题。OpenAI 团队在 GPT-2 的论文中还尝试解释无监督预训练在下游任务中取得良
好效果的原因：“由于特定任务的有监督学习目标与无监督学习目标（语言建模）
在本质上是相同的（预测下一个词元），主要区别就在于它们只是在全部训练数据
的子集上进行优化，因此对于特定下游任务而言，优化无监督的全局学习目标本
质上也是在优化有监督的任务学习目标” [17]。对这一说法的通俗理解是，语言
模型将每个（自然语言处理）任务都视为基于世界文本子集的下一个词预测问题。
因此，如果无监督语言建模经过训练后具有足够的能力复原全部世界文本，那么
本质上它就能够解决各种任务。这些 GPT-2 论文中的早期讨论与 Ilya Sutskever 在
接受 Jensen Huang 采访时的观点非常类似：“神经网络学到的是生成文本的过程中
的某种表示，这些模型的生成文本实际上是真实世界的投影……（语言模型）对
27
2.4 GPT 系列模型的技术演变
下一个单词的预测越准确，（对于世界知识）保真度就越高，在这个过程中获得的
分辨度就越高……”4。
2.4.2 规模扩展
虽然 GPT-2 的初衷是成为一个“无监督多任务学习器”，但在很多任务上与有
监督微调方法相比，模型效果整体上还是要逊色一些。在 GPT-2 基础上，GPT-3 针
对（几乎相同的）模型参数规模进行了大幅扩展，在下游任务中初步展现出了一
定的通用性（通过上下文学习技术适配下游任务），为后续打造更为强大的模型确
立了关键的技术发展路线。
• GPT-3. OpenAI 在 2020 年发布了 GPT-3 模型，将模型参数扩展到了 175B 的
规模。与 GPT-2 相比，GPT-3 直接将参数规模提升了 100 余倍，对于模型扩展在当
时给出了一个极限尝试，其雄心、魄力可见一斑。值得一提的是，OpenAI 的两篇
关于扩展法则的论文 [15, 21] 都是在 2020 年发表的，这说明在 GPT-3 开始训练时
可能已经进行了比较充分的实验探索，包括小版本模型的尝试、数据收集与清洗、
并行训练技巧等。在 GPT-3 的论文中，它正式提出了“上下文学习”这一概念，使
得大语言模型可以通过少样本学习的方式来解决各种任务。上下文学习可以指导
大语言模型学会“理解”自然语言文本形式描述的新任务，从而消除了针对新任
务进行微调的需要。基于这一学习范式，大语言模型的训练与利用可以通过语言
建模的形式进行统一描述：模型预训练是在给定上下文条件下预测后续文本序列，
模型使用则是根据任务描述以及示例数据来推理正确的任务解决方案。GPT-3 不
仅在各种自然语言处理任务中表现出了优异的效果，对于一些需要复杂推理能力
或领域适配能力的特定任务也具有较好的解决能力。虽然 GPT-3 的论文没有明确
提出上下文学习能力是大语言模型的涌现能力，但是指出了上下文学习对于大模
型的性能增益会更加显著，而对于小模型来说则收益较小（见 GPT-3 论文 [23] 的
原始图 1.2 ）。总体而言，GPT-3 可以被看作从预训练语言模型到大语言模型演进
过程中的一个重要里程碑，它证明了将神经网络扩展到超大规模可以带来大幅的
模型性能提升，并且建立了以提示学习方法为基础技术路线的任务求解范式。
4https://lifearchitect.ai/ilya/
28
2.4 GPT 系列模型的技术演变
2.4.3 能力增强
由于具有较强的模型性能，GPT-3 成为 OpenAI 开发更强大的大语言模型的
研究基础。根据公开资料披露的内容来说，OpenAI 探索了两种主要途径来改进
GPT-3 模型，即代码数据训练和人类偏好对齐。
• 代码数据训练. 原始的 GPT-3 模型的复杂推理任务能力仍然较弱，如对于编
程问题和数学问题的求解效果不好。为了解决这一问题，OpenAI 于 2021 年 7 月推
出了 Codex [47]，这是一个在大量 GitHub 代码数据集合上微调的 GPT 模型。实验
结果表明，Codex 可以解决非常困难的编程问题，还能显著提升大模型解决数学问
题的能力 [48]。此外，2022 年 1 月 OpenAI 还公开了一种用于训练文本和代码嵌入
的对比方法 [49]，结果表明该方法能够改善一系列相关任务的性能，包括线性探
测分类、文本搜索和代码搜索等。根据 OpenAI 所发布的 API 信息所示，GPT-3.5
模型是在基于代码训练的 GPT 模型（即 code-davinci-002）基础上开发的，这表
明在代码数据上进行训练有助于提高 GPT 模型的综合性能，尤其是代码能力。另
一个可能的启发是对于可用于预训练的数据范围的扩展，可能并不局限于自然语
言形式表达的文本数据。
• 人类对齐. OpenAI 关于人类对齐的公开研究工作可以追溯到 2017 年（实际
时间或更早）。在一篇题为“Learning from Human Preferences”5的博客文章中，OpenAI 的研究团队介绍了一项使用强化学习算法从人类标注的偏好数据中学习如何
改进模型性能的工作 [50]。在这篇强化学习工作发表不久，2017 年 7 月 OpenAI 研
究团队又提出了一项改进的强化学习算法— PPO 算法（Proximal Policy Optimization, PPO）[51]，这也成为了 OpenAI 在后续人类对齐技术里所采用的标配强化学
习算法。在 2020 年，OpenAI 研究团队将人类对齐算法应用于提升自然语言处理
任务上的能力，训练了一个根据人类偏好进行优化的摘要模型 [52]。以这些前期
工作为基础上，2022 年 1 月，OpenAI 正式推出 InstructGPT [28] 这一具有重要影
响力的学术工作，旨在改进 GPT-3 模型与人类对齐的能力，正式建立了基于人类
反馈的强化学习算法，即 RLHF 算法。值得一提的是，在 OpenAI 的论文和相关文
档中，很少使用“指令微调”（Instruction Tuning）一词，主要是使用“监督微调”
一词（即基于人类反馈的强化学习算法的第一步 [28]）。除了提高指令遵循能力，
基于人类反馈的强化学习算法有助于缓解有害内容的生成，这对于大语言模型在
实际应用中的安全部署非常重要。OpenAI 在一篇技术博客文章中描述了他们对齐
5https://openai.com/research/learning-from-human-preferences
29
2.4 GPT 系列模型的技术演变
研究的技术路线 [53]，并总结了三个有前景的研究方向：训练人工智能系统以达
到（1）使用人类反馈、（2）协助人类评估和（3）进行对齐研究。
通过这些增强技术，OpenAI 将改进后的具有更强能力的 GPT 模型命名为 GPT3.5 模型（参见第 3.1 节中有关 OpenAI API 的讨论）。
2.4.4 性能跃升
在历经上述近五年的重要探索，OpenAI 自 2022 年底开始发布了一系列重要
的技术升级，其中具有代表性的模型是 ChatGPT、GPT-4 以及 GPT-4V/GPT-4 Turbo，
这些模型极大提高了现有人工智能系统的能力水平，成为了大模型发展历程中的
重要里程碑。
• ChatGPT. 2022 年 11 月，OpenAI 发布了基于 GPT 模型的人工智能对话应用
服务 ChatGPT。OpenAI 官方博客文章 [54] 概要地介绍了 ChatGPT 的研发技术，主
要是沿用了 InstructGPT（原帖中称 ChatGPT 为“InstructGPT 的兄弟模型”）的训练
技术，但是对于对话能力进行了针对性优化。在训练数据的收集过程中，ChatGPT
将人类生成的对话数据（同时扮演用户和人工智能的角色）与训练 InstructGPT 的
相关数据进行结合，并统一成对话形式用于训练 ChatGPT。ChatGPT 在与人机对
话测试中展现出了众多的优秀能力：拥有丰富的世界知识、复杂问题的求解能力、
多轮对话的上下文追踪与建模能力、与人类价值观对齐的能力等。在后续的版本
更迭中，ChatGPT 进一步又支持了插件机制，通过现有工具或应用程序扩展了它
的功能，能够超越以往所有人机对话系统的能力水平。ChatGPT 一经推出就引发
了社会的高度关注，对于人工智能的未来研究产生了重要影响。
• GPT-4. 继 ChatGPT 后，OpenAI 于 2023 年 3 月发布了 GPT-4 [35]，它首次将
GPT 系列模型的输入由单一文本模态扩展到了图文双模态。总体来说，GPT-4 在
解决复杂任务方面的能力显著强于 GPT-3.5，在一系列面向人类的考试中都获得了
非常优异的结果。GPT-4 发布后，微软的研究团队针对其进行了大规模人类生成
问题的性能测试 [20]，实验结果表明 GPT-4 具有令人震撼的模型性能，论文作者
认为 GPT-4 的到来展现出了通用人工智能的曙光。此外，由于进行了为期六个月
的迭代对齐（在基于人类反馈的强化学习中额外增加了安全奖励信号），GPT-4 对
恶意或挑衅性查询的响应更加安全。在技术报告中 [35]，OpenAI 强调了安全开发
GPT-4 的重要性，并应用了一些干预策略来缓解大语言模型可能出现的问题——
幻觉、隐私泄露等。例如，研究人员引入了“红队攻击”（Red Teaming）机制 [55]
30
2.4 GPT 系列模型的技术演变
来减少生成有害或有毒的内容。更重要的是，GPT-4 搭建了完备的深度学习训练
基础架构，进一步引入了可预测扩展的训练机制，可以在模型训练过程中通过较
少计算开销来准确预测模型的最终性能。
• GPT-4V、GPT-4 Turbo以及多模态支持模型. 基于发布的GPT-4初版模型 [35]，
OpenAI 在 2023 年 9 月进一步发布了 GPT-4V，重点关注 GPT-4 视觉能力的安全
部署。在 GPT-4V 的系统说明中 [56]，广泛讨论了与视觉输入相关的风险评估手段
和缓解策略。GPT-4V 在多种应用场景中表现出了强大的视觉能力与综合任务解决
能力。在 2023 年 11 月，OpenAI 在开发者大会上发布了升级版的 GPT-4 模型，称
为 GPT-4 Turbo，引入了一系列技术升级：提升了模型的整体能力（比 GPT-4 更强
大），扩展了知识来源（拓展到 2023 年 4 月），支持更长上下文窗口（达到 128K），
优化了模型性能（价格更便宜），引入了若干新的功能（如函数调用、可重复输出
等）。同时，Assistants API 功能也被推出，旨在提升人工智能应用助手的开发效率，
开发人员可以利用特定的指令、外部知识和工具，在应用程序中快速创建面向特
定任务目标的智能助手。此外，新版本的 GPT 模型还进一步增强了多模态能力，
分别由 GPT-4 Turbo with Vision、DALL·E-3、TTS（Text-to-speech）以及 Listen to
voice samples 等支持实现。这些技术升级进一步提高了 GPT 模型的任务性能，扩
展了 GPT 模型的能力范围。更重要的是，随着模型性能和支撑功能的改进，极大
地加强了以 GPT 模型所形成的大模型应用生态系统。
尽管 GPT 系列模型取得了巨大的科研进展，这些最前沿的大语言模型仍然存
在一定的局限性。例如，GPT 模型可能在某些特定上下文中生成带有事实错误的
内容（即幻觉）或存在潜在风险的回应 [35]。更多针对大语言模型局限性的讨论
将在本书的第 12 章进行详细讨论。从人工智能的发展历程来看，开发能力更强、
更安全的大语言模型是一项长期的研究挑战。为了有效降低使用模型的潜在风险，
OpenAI 采用了迭代部署策略 [57]，通过多阶段开发和部署的生命周期来研发模型
与产品。


